{"cells":[{"cell_type":"code","execution_count":1,"source":["import sys\n","print(sys.version)\n","print(spark.version)"],"outputs":[{"output_type":"stream","name":"stdout","text":["3.8.5 | packaged by conda-forge | (default, Aug 29 2020, 01:22:49) \n","[GCC 7.5.0]\n","3.0.1\n"]}],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["import subprocess\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","\n","from pyspark.sql import SQLContext\n","sqlContext = SQLContext(sc)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":4,"source":["%%time\n","\n","influence_df = spark.read.parquet('gs://file-name')"],"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 3.78 ms, sys: 787 Âµs, total: 4.57 ms\n","Wall time: 6.62 s\n"]}],"metadata":{}},{"cell_type":"code","execution_count":5,"source":["influence_df.count()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["13672943"]},"metadata":{},"execution_count":5}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":8,"source":["influence_score = influence_df.select([influence_df.user_id, \n","                                       influence_df.retweeted_status,\n","                                       influence_df.retweet_count,\n","                                       influence_df.quote_count,\n","                                       influence_df.favorite_count])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":9,"source":["#create retweet count + quote count column and total engagement column\n","influence_score = influence_score\\\n","    .withColumn(\"retweet_quote_count\", col(\"retweet_count\")+col(\"quote_count\"))\\\n","    .withColumn(\"total_engagement\", col(\"retweet_count\")+col(\"quote_count\")+col(\"favorite_count\"))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":10,"source":["influence_score"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<table border='1'>\n","<tr><th>user_id</th><th>retweeted_status</th><th>retweet_count</th><th>quote_count</th><th>favorite_count</th><th>retweet_quote_count</th><th>total_engagement</th></tr>\n","<tr><td>56952466</td><td>[,, Wed Nov 03 03...</td><td>17</td><td>2</td><td>90</td><td>19</td><td>109</td></tr>\n","<tr><td>1377329837191405578</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n","<tr><td>290039056</td><td>[,, Tue Nov 02 22...</td><td>1312</td><td>39</td><td>3851</td><td>1351</td><td>5202</td></tr>\n","<tr><td>3646453759</td><td>[,, Wed Nov 03 04...</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr>\n","<tr><td>53567660</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n","<tr><td>309570860</td><td>[,, Tue Nov 02 21...</td><td>40</td><td>0</td><td>91</td><td>40</td><td>131</td></tr>\n","<tr><td>1921974067</td><td>[,, Tue Nov 02 19...</td><td>15950</td><td>1387</td><td>84938</td><td>17337</td><td>102275</td></tr>\n","<tr><td>1170715225756188672</td><td>[,, Tue Nov 02 21...</td><td>129</td><td>20</td><td>272</td><td>149</td><td>421</td></tr>\n","<tr><td>2875819794</td><td>[,, Tue Nov 02 14...</td><td>1218</td><td>59</td><td>5356</td><td>1277</td><td>6633</td></tr>\n","<tr><td>862342470360866816</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n","<tr><td>87624170</td><td>[,, Tue Nov 02 21...</td><td>6</td><td>0</td><td>13</td><td>6</td><td>19</td></tr>\n","<tr><td>3439396210</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n","<tr><td>1431502960056356872</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n","<tr><td>893700482971049984</td><td>[,, Tue Nov 02 23...</td><td>325</td><td>161</td><td>4738</td><td>486</td><td>5224</td></tr>\n","<tr><td>2320660100</td><td>[,, Wed Nov 03 02...</td><td>5</td><td>0</td><td>10</td><td>5</td><td>15</td></tr>\n","<tr><td>388641769</td><td>[,, Tue Nov 02 19...</td><td>15954</td><td>1402</td><td>84952</td><td>17356</td><td>102308</td></tr>\n","<tr><td>708382640</td><td>[,, Tue Oct 26 19...</td><td>27</td><td>1</td><td>39</td><td>28</td><td>67</td></tr>\n","<tr><td>1167159309668503553</td><td>[,, Tue Nov 02 23...</td><td>210</td><td>78</td><td>705</td><td>288</td><td>993</td></tr>\n","<tr><td>253110749</td><td>null</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n","<tr><td>18293186</td><td>[,, Mon Nov 01 13...</td><td>275</td><td>13</td><td>1169</td><td>288</td><td>1457</td></tr>\n","</table>\n","only showing top 20 rows\n"],"text/plain":["+-------------------+--------------------+-------------+-----------+--------------+-------------------+----------------+\n","|            user_id|    retweeted_status|retweet_count|quote_count|favorite_count|retweet_quote_count|total_engagement|\n","+-------------------+--------------------+-------------+-----------+--------------+-------------------+----------------+\n","|           56952466|[,, Wed Nov 03 03...|           17|          2|            90|                 19|             109|\n","|1377329837191405578|                null|            0|          0|             0|                  0|               0|\n","|          290039056|[,, Tue Nov 02 22...|         1312|         39|          3851|               1351|            5202|\n","|         3646453759|[,, Wed Nov 03 04...|            1|          0|             0|                  1|               1|\n","|           53567660|                null|            0|          0|             0|                  0|               0|\n","|          309570860|[,, Tue Nov 02 21...|           40|          0|            91|                 40|             131|\n","|         1921974067|[,, Tue Nov 02 19...|        15950|       1387|         84938|              17337|          102275|\n","|1170715225756188672|[,, Tue Nov 02 21...|          129|         20|           272|                149|             421|\n","|         2875819794|[,, Tue Nov 02 14...|         1218|         59|          5356|               1277|            6633|\n","| 862342470360866816|                null|            0|          0|             0|                  0|               0|\n","|           87624170|[,, Tue Nov 02 21...|            6|          0|            13|                  6|              19|\n","|         3439396210|                null|            0|          0|             0|                  0|               0|\n","|1431502960056356872|                null|            0|          0|             0|                  0|               0|\n","| 893700482971049984|[,, Tue Nov 02 23...|          325|        161|          4738|                486|            5224|\n","|         2320660100|[,, Wed Nov 03 02...|            5|          0|            10|                  5|              15|\n","|          388641769|[,, Tue Nov 02 19...|        15954|       1402|         84952|              17356|          102308|\n","|          708382640|[,, Tue Oct 26 19...|           27|          1|            39|                 28|              67|\n","|1167159309668503553|[,, Tue Nov 02 23...|          210|         78|           705|                288|             993|\n","|          253110749|                null|            0|          0|             0|                  0|               0|\n","|           18293186|[,, Mon Nov 01 13...|          275|         13|          1169|                288|            1457|\n","+-------------------+--------------------+-------------+-----------+--------------+-------------------+----------------+\n","only showing top 20 rows"]},"metadata":{},"execution_count":10}],"metadata":{}},{"cell_type":"code","execution_count":11,"source":["#create total tweet counts by user\n","total_counts = influence_score.groupBy('user_id').count().alias('total_count')"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":12,"source":["total_counts"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<table border='1'>\n","<tr><th>user_id</th><th>count</th></tr>\n","<tr><td>3305808923</td><td>38</td></tr>\n","<tr><td>40362526</td><td>2</td></tr>\n","<tr><td>404412177</td><td>45</td></tr>\n","<tr><td>1314113540</td><td>5</td></tr>\n","<tr><td>1667792210</td><td>1</td></tr>\n","<tr><td>1336451673053679620</td><td>44</td></tr>\n","<tr><td>200688599</td><td>328</td></tr>\n","<tr><td>1436036421437968391</td><td>1</td></tr>\n","<tr><td>3332152301</td><td>2</td></tr>\n","<tr><td>24198047</td><td>50</td></tr>\n","<tr><td>93628408</td><td>9</td></tr>\n","<tr><td>3314017610</td><td>1</td></tr>\n","<tr><td>90521228</td><td>9</td></tr>\n","<tr><td>1191181466090266634</td><td>8</td></tr>\n","<tr><td>632413348</td><td>185</td></tr>\n","<tr><td>50333838</td><td>5</td></tr>\n","<tr><td>404758600</td><td>409</td></tr>\n","<tr><td>3062054052</td><td>25</td></tr>\n","<tr><td>23254726</td><td>295</td></tr>\n","<tr><td>39922872</td><td>10</td></tr>\n","</table>\n","only showing top 20 rows\n"],"text/plain":["+----------+-----+\n","|   user_id|count|\n","+----------+-----+\n","| 319265497|  181|\n","| 528263964|    3|\n","| 145630849|   48|\n","|  59281238|   25|\n","|1331803164|  100|\n","|  16827256|  111|\n","| 112866168|   87|\n","|  23254726|  295|\n","| 306512683|   33|\n","|1350633384|   51|\n","| 455374803|   84|\n","| 404758600|  409|\n","|1444313197|  129|\n","|2842694614|   22|\n","|  17537826|   16|\n","| 753969769|    3|\n","|  14186094|    2|\n","|1530565592|   46|\n","|1551479125| 1069|\n","|  17516620|    2|\n","+----------+-----+\n","only showing top 20 rows"]},"metadata":{},"execution_count":12}],"metadata":{}},{"cell_type":"code","execution_count":13,"source":["#join both dataframes\n","joined_df = influence_score.join(total_counts, \"user_id\" ,\"left\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":14,"source":["joined_df2 = joined_df.withColumn(\"influence\", (col(\"retweet_quote_count\")/col(\"total_engagement\")) * col(\"count\"))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":15,"source":["#fill in nulls in influence columns\n","joined_df3 = joined_df2.na.fill(value=0,subset=[\"influence\"])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":16,"source":["joined_df4 = joined_df3.withColumn(\"influence\", when(joined_df3.retweeted_status.isNull(), joined_df3.influence*3).otherwise(joined_df3.influence))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":18,"source":["#joined_df4"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":19,"source":["influencers = joined_df4.select(['user_id','influence']).groupBy(\"user_id\").agg(avg(\"influence\").alias(\"avg_influence\"))"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["#influencers"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":37,"source":["influencers.count()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["2729351"]},"metadata":{},"execution_count":37}],"metadata":{}},{"cell_type":"code","execution_count":88,"source":["influencers.write.format(\"parquet\")\\\n",".mode('overwrite')\\\n",".save('gs://msca-bdp-students-bucket/shared_data/abharathsingh/influencers')\n","\n","print('done')"],"outputs":[{"output_type":"stream","name":"stdout","text":["done\n"]}],"metadata":{}},{"cell_type":"code","execution_count":38,"source":["joined_df5 = influencers.join(joined_df4, \n","                       \"user_id\",\n","                       \"left\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":39,"source":["joined_df5.count()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["13672943"]},"metadata":{},"execution_count":39}],"metadata":{}},{"cell_type":"code","execution_count":74,"source":["#drop variables that we don't need\n","joined_df6 = joined_df5.drop(\"retweet_count\")\\\n","    .drop(\"quote_count\")\\\n","    .drop(\"favorite_count\")\\\n","    .drop(\"influence\")\\\n","    .drop(\"retweeted_status\")\\\n","    .drop(\"retweet_quote_count\")\\\n","    .drop(\"total_engagement\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":75,"source":["joined_df6.printSchema()"],"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- user_id: long (nullable = true)\n"," |-- avg_influence: double (nullable = true)\n"," |-- count: long (nullable = true)\n","\n"]}],"metadata":{}},{"cell_type":"code","execution_count":76,"source":["joined_df7 = joined_df6.dropDuplicates(['user_id'])"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":77,"source":["joined_df7"],"outputs":[{"output_type":"execute_result","data":{"text/html":["<table border='1'>\n","<tr><th>user_id</th><th>avg_influence</th><th>count</th></tr>\n","<tr><td>964</td><td>0.0</td><td>2</td></tr>\n","<tr><td>38543</td><td>0.0</td><td>1</td></tr>\n","<tr><td>48763</td><td>0.4011765885616873</td><td>1</td></tr>\n","<tr><td>60033</td><td>0.0</td><td>3</td></tr>\n","<tr><td>747203</td><td>0.0</td><td>3</td></tr>\n","<tr><td>781154</td><td>0.0</td><td>3</td></tr>\n","<tr><td>806000</td><td>0.0</td><td>1</td></tr>\n","<tr><td>814261</td><td>1.6957981959907849</td><td>5</td></tr>\n","<tr><td>843931</td><td>0.4111409653584109</td><td>1</td></tr>\n","<tr><td>1070071</td><td>0.0</td><td>1</td></tr>\n","<tr><td>1469551</td><td>0.0</td><td>1</td></tr>\n","<tr><td>1709721</td><td>0.5915492957746479</td><td>1</td></tr>\n","<tr><td>1733741</td><td>4.9474985707281345</td><td>14</td></tr>\n","<tr><td>1810261</td><td>0.36585365853658536</td><td>2</td></tr>\n","<tr><td>2067961</td><td>0.35070277682550566</td><td>1</td></tr>\n","<tr><td>2262141</td><td>0.14676966292134833</td><td>3</td></tr>\n","<tr><td>3091161</td><td>0.0</td><td>1</td></tr>\n","<tr><td>3231091</td><td>0.0</td><td>1</td></tr>\n","<tr><td>4132381</td><td>0.25925925925925924</td><td>1</td></tr>\n","<tr><td>5023551</td><td>0.6154853483620606</td><td>4</td></tr>\n","</table>\n","only showing top 20 rows\n"],"text/plain":["+-------+-------------------+-----+\n","|user_id|      avg_influence|count|\n","+-------+-------------------+-----+\n","|    964|                0.0|    2|\n","|  38543|                0.0|    1|\n","|  48763| 0.4011765885616873|    1|\n","|  60033|                0.0|    3|\n","| 747203|                0.0|    3|\n","| 781154|                0.0|    3|\n","| 806000|                0.0|    1|\n","| 814261| 1.6957981959907849|    5|\n","| 843931| 0.4111409653584109|    1|\n","|1070071|                0.0|    1|\n","|1469551|                0.0|    1|\n","|1709721| 0.5915492957746479|    1|\n","|1733741|  4.947498570728135|   14|\n","|1810261|0.36585365853658536|    2|\n","|2067961|0.35070277682550566|    1|\n","|2262141|0.14676966292134833|    3|\n","|3091161|                0.0|    1|\n","|3231091|                0.0|    1|\n","|4132381|0.25925925925925924|    1|\n","|5023551| 0.6154853483620607|    4|\n","+-------+-------------------+-----+\n","only showing top 20 rows"]},"metadata":{},"execution_count":77}],"metadata":{}},{"cell_type":"code","execution_count":78,"source":["joined_df7.count()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["2729351"]},"metadata":{},"execution_count":78}],"metadata":{}},{"cell_type":"code","execution_count":87,"source":["from pyspark.sql import DataFrameStatFunctions as statFunc\n","statFunc(joined_df7).approxQuantile(\"avg_influence\", [0.5], 0)"],"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o428.approxQuantile.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(user_id#3L, 200), true, [id=#10062]\n+- Project [user_id#3L, retweeted_status#8, (retweet_count#9L + quote_count#7L) AS retweet_quote_count#49L, ((retweet_count#9L + quote_count#7L) + favorite_count#10L) AS total_engagement#56L]\n   +- FileScan parquet [user_id#3L,quote_count#7L,retweeted_status#8,retweet_count#9L,favorite_count#10L] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://msca-bdp-students-bucket/shared_data/abharathsingh/covid_tweets], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:bigint,quote_count:bigint,retweeted_status:struct<contributors:string,coordinates:...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:112)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:76)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3198)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.needsUnsafeRowConversion$lzycompute(DataSourceScanExec.scala:178)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.needsUnsafeRowConversion(DataSourceScanExec.scala:176)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:463)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:76)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:137)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:154)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 80 more\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-ef90ab73e646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrameStatFunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatFunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstatFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined_df7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg_influence\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   2250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m     \u001b[0mapproxQuantile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1985\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1986\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1987\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o428.approxQuantile.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(user_id#3L, 200), true, [id=#10062]\n+- Project [user_id#3L, retweeted_status#8, (retweet_count#9L + quote_count#7L) AS retweet_quote_count#49L, ((retweet_count#9L + quote_count#7L) + favorite_count#10L) AS total_engagement#56L]\n   +- FileScan parquet [user_id#3L,quote_count#7L,retweeted_status#8,retweet_count#9L,favorite_count#10L] Batched: false, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://msca-bdp-students-bucket/shared_data/abharathsingh/covid_tweets], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:bigint,quote_count:bigint,retweeted_status:struct<contributors:string,coordinates:...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:112)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:76)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3200)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3198)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:102)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:529)\n\tat scala.None$.get(Option.scala:527)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.needsUnsafeRowConversion$lzycompute(DataSourceScanExec.scala:178)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.needsUnsafeRowConversion(DataSourceScanExec.scala:176)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:463)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:76)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:137)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:154)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 80 more\n"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":79,"source":["#join onto dataframe to have all new columns\n","covid_influence = influence_df.join(joined_df7, \n","                       \"user_id\",\n","                       \"left\")"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":80,"source":["covid_influence.schema.names"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["['user_id',\n"," 'created_at',\n"," 'id',\n"," 'user',\n"," 'user_name',\n"," 'user_description',\n"," 'followers_count',\n"," 'quote_count',\n"," 'retweeted_status',\n"," 'retweet_count',\n"," 'favorite_count',\n"," 'text',\n"," 'verified_user',\n"," 'avg_influence',\n"," 'count']"]},"metadata":{},"execution_count":80}],"metadata":{}},{"cell_type":"code","execution_count":82,"source":["#covid_influence"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":83,"source":["covid_influence.count()"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["13672943"]},"metadata":{},"execution_count":83}],"metadata":{}},{"cell_type":"code","execution_count":84,"source":["covid_influence.write.format(\"parquet\")\\\n",".mode('overwrite')\\\n",".save('gs://folder-name')\n","print('done')"],"outputs":[{"output_type":"stream","name":"stdout","text":["done\n"]}],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}